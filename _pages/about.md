---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I am a Research Scientist in the TCL AI Lab at [TCL Research, Hong Kong](https://www.linkedin.com/company/tcl-corporate-research-hk-co-ltd/mycompany/)  where I work on Computer Vision, Machine Learning, and Deep Learning. I received my Ph.D. degree in electrical engineering from the [City University of Hong Kong](https://www.cityu.edu.hk/), in 2019. During my Ph.D. study, I worked under the supervision of [Dr.Lai-Man Po](http://www.ee.cityu.edu.hk/~lmpo/) on Face Liveness detection using Deep Learning techniques. Prior to my Ph.D. degree, I worked in NuSyS Lab under the supervision of [Dr. Muhammad Tariq](https://sites.google.com/a/nu.edu.pk/mtariq/home) on Wireless Multimedia Sensor Networks  (WMSN). I also worked as an Algorithm Specialist with [TCL Corporate Research (Hong Kong) co., Limited](http://tclrd.com.hk/) before assuming duties as a Research Scientist at the same center in 2021.

Research Interests
======
Deep Learning and Computer Vision, Federated Learning, Video Understanding, Computational Photography, Audio Understanding

Available Positions
======
**Software Engineer (Intern)**

**Software Engineering Intern – Smart Homes at TCL**  
TCL is seeking a motivated **Software Engineering Intern** to join our team and contribute to developing innovative smart home solutions. In this role, you’ll work on designing, testing, and optimizing software for IoT devices, cloud integration, and user-friendly interfaces. Candidates who are pursuing a degree in Computer Science, Software Engineering, or a related field, with proficiency in Python, Java, or C++, and a strong interest in IoT and smart home innovation are encouraged to apply. This is a hands-on opportunity to gain real-world experience in a fast-paced, innovative environment, with potential for future full-time opportunities. If you’re excited about shaping the future of smart homes, send your resume to **[yasar@tcl.com]**. 




<!--
Collaborators
=====
[Nicholas Lane](http://niclane.org/) (Professor in the  Department of Computer Science and Technology at the University of Cambridge) <br>
[Muhammad Tariq](https://sites.google.com/a/nu.edu.pk/mtariq/home) (Professor at FAST NUCES Islamabad) <br>
[JiaJun Shen](https://scholar.google.com/citations?hl=en&user=qckHL1AAAAAJ&view_op=list_works&sortby=pubdate) (Current: Deep Mind) (Previous: Chief AI Scientist at TCL)<br> 
[Pedro Porto Buarque de Gusmão](https://portobgusmao.com/) (Cambridge Machine Learning Systems Lab) <br>
[Yan Gao](https://scholar.google.com/citations?hl=en&user=_im5GrcAAAAJ&view_op=list_works&sortby=pubdate) (University of Cambridge) <br>
-->


News 
====== 
1. **[2024]** Our paper titled [Exploring Federated Self-Supervised Learning for General Purpose Audio Understanding](https://arxiv.org/abs/2402.02889) has been accepted to the ICASSP-2024 workshop on Self-supervision in Audio, Speech and Beyond. <a href="https://arxiv.org/abs/2402.02889"> [Preprint] </a>   
2. **[2024]** Our paper titled  [AudioRepInceptionNeXt: A lightweight single-stream architecture for efficient audio recognition](https://www.sciencedirect.com/science/article/abs/pii/S0925231224002030) has been accepted in NeuroComputing <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4588783"> [Preprint] </a> <a href="https://github.com/StevenLauHKHK/AudioRepInceptionNeXt"> [Code] </a>.
3. **[2023]** Our paper titled [Large Separable Kernel Attention: Rethinking the Large Kernel Attention Design in CNN](https://www.sciencedirect.com/science/article/abs/pii/S0957417423018547#:~:text=However%2C%20in%20a%20recent%20study,et%20al.%2C%202022) has been accepted in ESWA <a href="https://arxiv.org/abs/2309.01439"> [Preprint]</a> <a href="https://github.com/stevenlauhkhk/large-separable-kernel-attention"> [Code] </a>. 
4. **[2023]** Our paper titled [L-DAWA: Layer-wise Divergence Aware Weight Aggregation in Federated Self-Supervised Visual Representation Learning](https://arxiv.org/pdf/2307.07393.pdf) has been accepted in ICCV-2023 <a href="https://arxiv.org/pdf/2307.07393.pdf">[Preprint]</a> <a href="/files/ICCV2023_image_SSL_FL__supplementary_.pdf"> [Supplementary Materials]</a>.
5. **[2023]** Our solution got first place award in [EPIC-SOUNDS Audio-Based Interaction Recognition](https://codalab.lisn.upsaclay.fr/competitions/9729#results)
6. **[2023]** A short highlight on using federated learning with self-supervision for video understanding is now available on the [Flower Blogs](https://flower.dev/blog/2023-04-05-federated-learning-with-self-supervision)
7. **[2022]**  Our paper titled [Federated Self-Supervised Learning for Video Understanding](https://arxiv.org/abs/2207.01975) has been accepted in ECCV-2022
8. **[2022]** Video of my short talk on Federated Learning with Self-Supervision at the Flower Summit 2022 is now available on the [Flower YouTube Channel](https://www.youtube.com/watch?v=ZLqst0lVte8&t=212s)
9. **[2022]** A paper is accepted in [L3D-IVU - CVPR2022](href=https://sites.google.com/view/l3d-ivu/)
10. **[2021]** Our paper titled [VCGAN: Video Colorization with Generative Adversarial Networks](https://arxiv.org/pdf/2104.12357.pdf) has been accepted for publication in IEEE Transactions on Multimedia
11. **[2021]** Our book chapter titled Visual Information Processing and Transmission in Wireless Multimedia Sensor Networks: A Deep Learning-Based Practical Approach has been accepted for publication in the upcoming book [Internet of Multimedia Things (IoMT):  Techniques and Applications](https://www.elsevier.com/books/internet-of-multimedia-things-iomt-techniques-and-applications/shukla/978-0-323-85845-8)

Reviewer
======
1. **Journals:** IEEE TCSVT, IEEE Access, ESWA, JVCI, SPIC, IEEE TNNLS
2. **Conferences:** IEEE CVPR-2024, IEEE ICET, IEEE INMIC, CVPR-FedVision (2023-2024)

